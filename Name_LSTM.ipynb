{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "import unidecode\n",
    "from torch.utils.tensorboard import SummaryWriter # This is to print to tensorBorad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get characters from string.printable\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read large text file (Note can be any text file: not limited to just names)\n",
    "file = unidecode.unidecode(open(\"data/names.txt\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embed(x)\n",
    "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
    "        out = self.fc(out.reshape(out.shape[0], -1))\n",
    "        return out, (hidden, cell)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.chunk_len = 250\n",
    "        self.num_epochs = 5000\n",
    "        self.batch_size = 1\n",
    "        self.print_every = 50\n",
    "        self.hidden_size = 256\n",
    "        self.num_layers = 2\n",
    "        self.lr = 0.003\n",
    "\n",
    "    def char_tensor(self, string):\n",
    "        tensor = torch.zeros(len(string)).long()\n",
    "        for c in range(len(string)):\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "        return tensor\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        start_idx = random.randint(0, len(file) - self.chunk_len)\n",
    "        end_idx = start_idx + self.chunk_len + 1\n",
    "        text_str = file[start_idx:end_idx]\n",
    "        text_input = torch.zeros(self.batch_size, self.chunk_len)\n",
    "        text_target = torch.zeros(self.batch_size, self.chunk_len)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i, :] = self.char_tensor(text_str[:-1])\n",
    "            text_target[i, :] = self.char_tensor(text_str[1:])\n",
    "\n",
    "        return text_input.long(), text_target.long()\n",
    "\n",
    "    def generate(self, initial_str=\"A\", predict_len=100, temperature=0.85):\n",
    "        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "        initial_input = self.char_tensor(initial_str)\n",
    "        predicted = initial_str\n",
    "\n",
    "        for p in range(len(initial_str) - 1):\n",
    "            _, (hidden, cell) = self.rnn(\n",
    "                initial_input[p].view(1).to(device), hidden, cell\n",
    "            )\n",
    "\n",
    "        last_char = initial_input[-1]\n",
    "\n",
    "        for p in range(predict_len):\n",
    "            output, (hidden, cell) = self.rnn(\n",
    "                last_char.view(1).to(device), hidden, cell\n",
    "            )\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_char = torch.multinomial(output_dist, 1)[0]\n",
    "            predicted_char = all_characters[top_char]\n",
    "            predicted += predicted_char\n",
    "            last_char = self.char_tensor(predicted_char)\n",
    "\n",
    "        return predicted\n",
    "\n",
    "    # input_size, hidden_size, num_layers, output_size\n",
    "    def train(self):\n",
    "        self.rnn = RNN(\n",
    "            n_characters, self.hidden_size, self.num_layers, n_characters\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n",
    "\n",
    "        print(\"=> Starting training\")\n",
    "\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            inp, target = self.get_random_batch()\n",
    "            hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "\n",
    "            self.rnn.zero_grad()\n",
    "            loss = 0\n",
    "            inp = inp.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            for c in range(self.chunk_len):\n",
    "                output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n",
    "                loss += criterion(output, target[:, c])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item() / self.chunk_len\n",
    "\n",
    "            if epoch % self.print_every == 0:\n",
    "                print(f\"Loss: {loss}\")\n",
    "                print(self.generate())\n",
    "\n",
    "            writer.add_scalar(\"Training loss\", loss, global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting training\n",
      "Loss: 2.44514794921875\n",
      "A\n",
      "Jhlacis\n",
      "Jasdia\n",
      "Kesabhaan\n",
      "Smaya\n",
      "Tegine\n",
      "Cheleny\n",
      "Kacdiagon\n",
      "Langay\n",
      "Jarash\n",
      "Jyrya\n",
      "Larurh\n",
      "Manon\n",
      "Jison\n",
      "Aosa\n",
      "Loss: 2.093169677734375\n",
      "Axelly\n",
      "Elgerz\n",
      "Laan\n",
      "Jola\n",
      "Auneth\n",
      "Jemer\n",
      "Ca\n",
      "ami\n",
      "Fartka\n",
      "Ger\n",
      "erre\n",
      "Baric\n",
      "Hosa\n",
      "Andtol\n",
      "Carri\n",
      "Taydra\n",
      "Lara\n",
      "Vosia\n",
      "Loss: 2.036279541015625\n",
      "Alanie\n",
      "Ruxennie\n",
      "Lrigenda\n",
      "Brian\n",
      "Yogine\n",
      "Arnett\n",
      "Broria\n",
      "Kariey\n",
      "Aineh\n",
      "Shishah\n",
      "Relstia\n",
      "Jarie\n",
      "tabme\n",
      "Laline\n",
      "L\n",
      "Loss: 1.96249560546875\n",
      "A\n",
      "Deliandah\n",
      "Starkie\n",
      "Lara\n",
      "Lonviw\n",
      "Jena\n",
      "Trattan\n",
      "Frren\n",
      "Ellico\n",
      "Efnisy\n",
      "Elfarina\n",
      "crilen\n",
      "Alicha\n",
      "Jasallie\n",
      "Cad\n",
      "\n",
      "Loss: 1.997978271484375\n",
      "Avy\n",
      "Kyn\n",
      "La\n",
      "Dahro\n",
      "Drend\n",
      "Danelin\n",
      "Kayl\n",
      "Ariell\n",
      "Marli\n",
      "Joriel\n",
      "Estoe\n",
      "Sheon\n",
      "Golie\n",
      "Anne\n",
      "Shistis\n",
      "Kabrie\n",
      "Tatian\n",
      "\n",
      "Loss: 2.0043060302734377\n",
      "Apon\n",
      "Sdern\n",
      "Javia\n",
      "Geonne\n",
      "Arberes\n",
      "Jeranda\n",
      "Noil\n",
      "Malles\n",
      "Navug\n",
      "Cairmen\n",
      "Jachela\n",
      "Comris\n",
      "Leneg\n",
      "Gerry\n",
      "Tared\n",
      "Al\n",
      "Loss: 1.82191748046875\n",
      "Annk\n",
      "Lancie\n",
      "Marianna\n",
      "Norga\n",
      "Charina\n",
      "Billan\n",
      "Alanna\n",
      "Kathan\n",
      "Tonny\n",
      "Janha\n",
      "Linmon\n",
      "Itae\n",
      "Rosane\n",
      "Charlina\n",
      "ornia\n",
      "Loss: 2.043576416015625\n",
      "Andra\n",
      "Luna\n",
      "Heli\n",
      "Karin\n",
      "Ryseth\n",
      "Karia\n",
      "Kandy\n",
      "Genan\n",
      "Roben\n",
      "Emero\n",
      "Lamin\n",
      "Uleden\n",
      "Marici\n",
      "Chrnis\n",
      "Linah\n",
      "Raman\n",
      "Kil\n",
      "Loss: 1.8744019775390626\n",
      "Ana\n",
      "Kathrin\n",
      "Trynd\n",
      "Joulia\n",
      "Jacky\n",
      "Dennis\n",
      "Micaude\n",
      "Marianna\n",
      "Jarian\n",
      "Dolan\n",
      "Carlie\n",
      "Sallie\n",
      "Kiandra\n",
      "Dorella\n",
      "Pat\n",
      "Loss: 1.784819091796875\n",
      "Ath\n",
      "Albeth\n",
      "Roxanni\n",
      "Rosond\n",
      "Mara\n",
      "Stenia\n",
      "Latry\n",
      "Janisa\n",
      "Karova\n",
      "Kattuete\n",
      "Kassia\n",
      "Rolly\n",
      "Ednya\n",
      "Nola\n",
      "Tona\n",
      "Sam\n",
      "D\n",
      "Loss: 1.929945556640625\n",
      "Anl\n",
      "Glowe\n",
      "Shiry\n",
      "Arikae\n",
      "Bendrey\n",
      "Mila\n",
      "Sazami\n",
      "Tymie\n",
      "Grandley\n",
      "Corry\n",
      "Les\n",
      "Zaida\n",
      "Jamina\n",
      "Kathleina\n",
      "Addeline\n",
      "A\n",
      "Loss: 1.825932373046875\n",
      "Amilie\n",
      "Tra\n",
      "Michell\n",
      "Gunsio\n",
      "olly\n",
      "Colma\n",
      "Kistina\n",
      "Mariel\n",
      "Johnnin\n",
      "Marolyn\n",
      "Dadie\n",
      "Lorie\n",
      "Maslie\n",
      "Ployah\n",
      "Skivie\n",
      "\n",
      "Loss: 1.780550537109375\n",
      "Aver\n",
      "Machelle\n",
      "Haiur\n",
      "Willy\n",
      "Danna\n",
      "Nathace\n",
      "Sadley\n",
      "Wundrian\n",
      "Sharhian\n",
      "Adrene\n",
      "Conton\n",
      "Makary\n",
      "Bardon\n",
      "Prakah\n",
      "D\n",
      "Loss: 1.612661376953125\n",
      "Alida\n",
      "Lewi\n",
      "Ady\n",
      "Kary\n",
      "Lilly\n",
      "Cark\n",
      "Monie\n",
      "Hailey\n",
      "Laralma\n",
      "Hilima\n",
      "Lene\n",
      "Janie\n",
      "Seanna\n",
      "Jawanna\n",
      "Ena\n",
      "Burtari\n",
      "Ashl\n",
      "Loss: 1.61955029296875\n",
      "Ashiah\n",
      "Jabel\n",
      "Mabya\n",
      "Dawnen\n",
      "Jannette\n",
      "Krynt\n",
      "Traces\n",
      "Carlah\n",
      "Temuel\n",
      "Katrick\n",
      "Bencella\n",
      "Sesty\n",
      "Kelly\n",
      "Thearra\n",
      "Ja\n",
      "Loss: 1.614223388671875\n",
      "Alian\n",
      "Jay\n",
      "Somie\n",
      "Kayden\n",
      "Celetor\n",
      "Marca\n",
      "Khoni\n",
      "Colt\n",
      "Triston\n",
      "Caley\n",
      "Tayda\n",
      "Marko\n",
      "Ashley\n",
      "Wimmio\n",
      "Terrick\n",
      "Tyron\n",
      "Loss: 1.5146820068359375\n",
      "Allys\n",
      "Destin\n",
      "Gewen\n",
      "Anyster\n",
      "Christin\n",
      "Samar\n",
      "Anny\n",
      "Tam\n",
      "Alena\n",
      "Petter\n",
      "Gradlie\n",
      "Tastael\n",
      "Cuntor\n",
      "Lorace\n",
      "Mark\n",
      "Ev\n",
      "Loss: 1.4596380615234374\n",
      "Abey\n",
      "Alanalena\n",
      "Oriam\n",
      "Julee\n",
      "Millard\n",
      "Luian\n",
      "Jimon\n",
      "Jozky\n",
      "Jofian\n",
      "ReEAntery\n",
      "Oy\n",
      "Aulenia\n",
      "AgANNichola\n",
      "EugNigCo\n",
      "Loss: 1.78495068359375\n",
      "A(del\n",
      "Lian\n",
      "Rosa\n",
      "Peudra\n",
      "Roberta\n",
      "Tammer\n",
      "Carmen\n",
      "Charleen\n",
      "Rachel\n",
      "Jeanne\n",
      "Rucell\n",
      "Jana\n",
      "Teaguel\n",
      "Pammia\n",
      "Carlea\n",
      "Loss: 1.351927001953125\n",
      "Ade\n",
      "Allissa\n",
      "Alison\n",
      "Bridge\n",
      "Cheris\n",
      "Cinny\n",
      "Ry\n",
      "Lindsey\n",
      "Tracko\n",
      "Clari\n",
      "oricia\n",
      "Duan\n",
      "Dristin\n",
      "Lorey\n",
      "Kenthia\n",
      "Cali\n",
      "Loss: 1.4776563720703124\n",
      "Adrich\n",
      "Jacquel\n",
      "Adrian\n",
      "August\n",
      "Alegs\n",
      "Emmul\n",
      "Thendor\n",
      "Dona\n",
      "Jaqmen\n",
      "Donzie\n",
      "Mardy\n",
      "Maggie\n",
      "Annika\n",
      "Bryn\n",
      "Dana\n",
      "Edd\n",
      "Loss: 1.6006414794921875\n",
      "Arna\n",
      "Elisa\n",
      "Pegin\n",
      "Tana\n",
      "Burney\n",
      "Iramow\n",
      "Davis\n",
      "Ednett\n",
      "Marven\n",
      "Geodara\n",
      "Brit\n",
      "Lizaw\n",
      "Margar\n",
      "Marichela\n",
      "Berny\n",
      "Len\n",
      "Loss: 1.8497330322265626\n",
      "Anna\n",
      "Elina\n",
      "Judy\n",
      "Marlen\n",
      "Ellani\n",
      "Delora\n",
      "Devin\n",
      "Jayanne\n",
      "Ellera\n",
      "Aubra\n",
      "Mayden\n",
      "Rachelle\n",
      "Rose\n",
      "Shavin\n",
      "Stacie\n",
      "Ra\n",
      "Loss: 1.31640087890625\n",
      "Anna\n",
      "Braydian\n",
      "Alissa\n",
      "Rayara\n",
      "xabilla\n",
      "Karen\n",
      "Corael\n",
      "Cessie\n",
      "Charo\n",
      "Dari\n",
      "Dianna\n",
      "Joxanda\n",
      "Margara\n",
      "Janetia\n",
      "Jan\n",
      "Loss: 1.2530113525390625\n",
      "Antrista\n",
      "Darrett\n",
      "Eva\n",
      "Frantin\n",
      "Elaison\n",
      "Evelyn\n",
      "Emma\n",
      "Elsie\n",
      "Emmett\n",
      "Francisa\n",
      "Gabenda\n",
      "Gusten\n",
      "Iria\n",
      "Charlotte\n",
      "\n",
      "Loss: 2.319220458984375\n",
      "Alene\n",
      "Collen\n",
      "Kathleen\n",
      "Lorson\n",
      "Madelyn\n",
      "Frank\n",
      "Veronia\n",
      "Clarine\n",
      "Manna\n",
      "Shani\n",
      "Sharitt\n",
      "Vaden\n",
      "Marvin\n",
      "Marcell\n",
      "E\n",
      "Loss: 1.538416015625\n",
      "Andra\n",
      "Arika\n",
      "Nelly\n",
      "Adilyn\n",
      "Villie\n",
      "Elisabeth\n",
      "Margaret\n",
      "Tanisha\n",
      "Alison\n",
      "Ananja\n",
      "Annett\n",
      "Brastany\n",
      "Ivanya\n",
      "Jesse\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m gennames \u001b[38;5;241m=\u001b[39m Generator()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgennames\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mGenerator.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m     output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(inp[:, c], hidden, cell)\n\u001b[0;32m     77\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(output, target[:, c])\n\u001b[1;32m---> 79\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     81\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_len\n",
      "File \u001b[1;32m~\\.conda\\envs\\deepLearning\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deepLearning\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gennames = Generator()\n",
    "gennames.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05a35dc2fd6d4dec1ed9414f7f6328dda63cb5a42ac9d5af0bb2c5df49181232"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
